{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NbTWMgKTgXMb"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "#from umap import UMAP\n",
    "#from sklearn.decomposition import PCA\n",
    "#%matplotlib notebook\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, \\\n",
    "                            precision_score, recall_score, accuracy_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V83pEivdgZKQ"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aj1cRhZcgbEL"
   },
   "outputs": [],
   "source": [
    "def encodeString(s, c_freqs) -> list:\n",
    "    res = []\n",
    "    for c in s:\n",
    "        res.append(c_freqs[c])\n",
    "    return res\n",
    "\n",
    "# def encodeString2(s) -> list:\n",
    "#     res = np.zeros(len(c_freqs)*7)\n",
    "#     for i, c in enumerate(s):\n",
    "#         res[c_freqs[c] + len(c_freqs) * i] = 1\n",
    "#     return res\n",
    "\n",
    "def decodeString(s, c_freqs) -> list:\n",
    "    res = []\n",
    "    for c in s:\n",
    "        for c2, k2 in c_freqs.items():\n",
    "            if k2 == c:\n",
    "                res.append(c2)\n",
    "                break\n",
    "    return \"\".join(res)\n",
    "\n",
    "# Это 7-граммы\n",
    "def extractNGrammsFromFile(filename:str, soi, freq_thr=1) -> (dict, dict):\n",
    "    with open(filename) as infile:\n",
    "        test_text = infile.read(200000000)\n",
    "    for c in tqdm(\".,?!:;\\t\\n\\r/[{(<>)}]\\\"'@#$%^*_+&0123456789=\", desc='Replacement'):\n",
    "        test_text = test_text.replace(c, \" \")\n",
    "    for i in tqdm(range(16), desc='Replacement'):\n",
    "        test_text = test_text.replace(\"  \", \" \")\n",
    "    test_text = test_text.lower()\n",
    "        \n",
    "    words = test_text.split(\" \")\n",
    "    word_freqs = defaultdict((int))\n",
    "    for word in tqdm(words, desc='Creating n-gramms'):\n",
    "        if len(word) < 4:\n",
    "            continue\n",
    "        cur = \"   \" + word[:4]\n",
    "        if cur[3] in soi:\n",
    "            word_freqs[cur] += 1\n",
    "        for c in word[4:]:\n",
    "            cur = cur[1:] + c\n",
    "            if cur[3] in soi:\n",
    "                word_freqs[cur] += 1\n",
    "        for i in [1,2,3]:\n",
    "            cur = cur[1:] + \" \"\n",
    "            if cur[3] in soi:\n",
    "                word_freqs[cur] += 1\n",
    "                \n",
    "    to_del = []\n",
    "    for word, freq in word_freqs.items():\n",
    "        #if freq<10:\n",
    "        if freq < freq_thr:\n",
    "            to_del.append(word)\n",
    "\n",
    "    for word in to_del:\n",
    "        del word_freqs[word]\n",
    "    to_del = []\n",
    "    \n",
    "    c_freqs = Counter(test_text[:10000000])\n",
    "    # Можно обойтись без сортировки, если что.\n",
    "    s_freqs = sorted([(c, f) for c, f in c_freqs.items() if f>2], key=lambda x: x[1], reverse=True)\n",
    "    c_freqs = {c:i for i, c in enumerate([x[0] for x in s_freqs])}\n",
    "    return word_freqs, c_freqs\n",
    "\n",
    "replaces = {\"ä\":\"a\", \"Ä\":\"A\", \"à\":\"a\", \"À\":\"A\",\"â\":\"a\", \"Â\":\"A\", \"á\":\"a\", \"Á\":\"A\",\"ā\":\"a\", \"Ā\":\"A\", \"ă\":\"a\", \"Ă\":\"A\", \\\n",
    "            \"č\":\"c\", \"Č\":\"C\", \"ć\":\"c\", \"Ć\":\"C\", \"ç\":\"c\", \"Ç\":\"C\", \\\n",
    "            \"đ\":\"d\", \"Ð\": \"D\", \"ď\":\"d\", \"Ď\":\"D\", \\\n",
    "            \"é\":\"e\", \"É\":\"E\", \"è\":\"e\", \"È\":\"E\", \"ê\":\"e\", \"Ê\":\"E\", \"ē\":\"e\", \"Ē\":\"E\", \\\n",
    "            \"ğ\":\"g\", \"Ğ\":\"G\", \"ģ\":\"g\", \"Ģ\":\"G\", \\\n",
    "            \"î\":\"i\", \"Î\":\"I\", \"ī\":\"i\", \"Ī\":\"I\", \"í\":\"i\", \"Í\":\"I\", \\\n",
    "            \"ķ\":\"k\", \"Ķ\":\"K\", \\\n",
    "            \"ļ\":\"l\", \"Ļ\":\"L\", \"ĺ\":\"l\", \"Ĺ\":\"L\", \"ľ\":\"l\", \"Ľ\":\"L\", \\\n",
    "            \"ņ\":\"n\", \"Ņ\":\"N\", \"ň\":\"n\", \"Ň\":\"N\", \\\n",
    "            \"ö\":\"o\", \"Ö\":\"O\", \"ô\":\"o\", \"Ô\":\"O\", \"ó\":\"o\", \"Ó\":\"O\", \\\n",
    "            \"ŕ\":\"r\", \"Ŕ\":\"R\", \\\n",
    "            \"ş\":\"s\", \"Ş\":\"S\", \"š\":\"s\", \"Š\":\"S\", \\\n",
    "            \"ț\":\"t\", \"Ț\":\"T\", \"ť\":\"t\", \"Ť\":\"T\", \\\n",
    "            \"ü\":\"u\", \"Ü\":\"U\", \"ù\":\"u\", \"Ù\":\"U\", \"ū\":\"u\", \"Ū\":\"U\", \"û\":\"u\", \"Û\":\"U\", \"ú\":\"u\", \"Ú\":\"U\", \\\n",
    "            \"ý\":\"y\", \"Ý\":\"Y\", \\\n",
    "            \"ž\":\"z\", \"Ž\":\"Z\"\n",
    "           }\n",
    "\n",
    "symbols_of_interest = list(replaces.keys())\n",
    "\n",
    "def makeNumericalVectors(word_freqs: dict, c_freqs: dict) -> (list, list):\n",
    "    vectors = []\n",
    "    classes = []\n",
    "    for word in tqdm(word_freqs.keys(), desc='Vectorizing words'):\n",
    "        try:\n",
    "            if word[3] in symbols_of_interest:\n",
    "                c = 1\n",
    "                word2 = word[:3] + replaces[word[3]] + word[4:]\n",
    "            else:\n",
    "                c = 0\n",
    "                word2 = word\n",
    "            e = encodeString(word2, c_freqs) \n",
    "            for i in range(word_freqs[word]):\n",
    "                vectors.append(e)\n",
    "                classes.append(c)\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "    return vectors, classes\n",
    "\n",
    "def makeNormalizedNumericalVectors(word_freqs: dict, c_freqs: dict) -> (list, list):\n",
    "    total = sum(c_freqs.values())\n",
    "    for k in c_freqs.keys():\n",
    "        c_freqs[k] /= total\n",
    "    return makeNumericalVectors(word_freqs, c_freqs)\n",
    "\n",
    "def makeData4GeneratorsAndSplit(word_freqs: dict, c_freqs: dict, percent: float) -> (dict, dict):\n",
    "    thr = sum(word_freqs.values()) * percent\n",
    "    prc = 0\n",
    "    test_words = {}\n",
    "    train_words = {}\n",
    "    len1 = len(word_freqs)\n",
    "    words1 = list(word_freqs.keys())\n",
    "    \n",
    "    fail = 0\n",
    "    while prc < thr and fail < 20:\n",
    "        index = random.randint(0, len1 - 1)\n",
    "        w = words1[index]\n",
    "        try:\n",
    "            e = encodeString(w, c_freqs) \n",
    "        except Exception:\n",
    "            del words1[index]\n",
    "            len1 -= 1\n",
    "            continue\n",
    "        if w in test_words.keys():\n",
    "            fail += 1\n",
    "            continue\n",
    "        test_words[w] = word_freqs[w]\n",
    "        prc += word_freqs[w]\n",
    "        fail = 0\n",
    "        \n",
    "    for w, f in word_freqs.items():\n",
    "        if w not in test_words.keys():\n",
    "            try:\n",
    "                e = encodeString(w, c_freqs) \n",
    "            except Exception:\n",
    "                continue\n",
    "            train_words[w] = f\n",
    "\n",
    "    return train_words, test_words\n",
    "\n",
    "def makeData4GeneratorsAndSplit2(word_freqs: dict, c_freqs: dict, percent: float) -> (dict, dict):\n",
    "    thr = sum(word_freqs.values()) * percent\n",
    "    prc = 0\n",
    "    test_words = defaultdict((int))\n",
    "    train_words = deepcopy(word_freqs)\n",
    "\n",
    "    for w in word_freqs:\n",
    "        try:\n",
    "            e = encodeString(w, c_freqs) \n",
    "        except Exception:\n",
    "            del train_words[w]\n",
    "\n",
    "    len1 = len(train_words)\n",
    "    words1 = list(train_words.keys())\n",
    "    freqs1 = list(train_words.values())\n",
    "    sum_words = sum(freqs1)\n",
    "    \n",
    "    while prc < thr:\n",
    "        index = random.randint(0, sum_words-1)\n",
    "        i, j = 0, 0\n",
    "        while i<len1 and j<index:\n",
    "          j += freqs1[i]\n",
    "          i += 1\n",
    "        if i == len1:\n",
    "          i -= 1\n",
    "        w = words1[i]\n",
    "            \n",
    "        if freqs1[i] > 10:\n",
    "          delta = int(freqs1[i] * 0.1)\n",
    "        else:\n",
    "          delta = 1\n",
    "        test_words[w] += delta\n",
    "        train_words[w] -= delta\n",
    "        freqs1[i] -= delta\n",
    "        if train_words[w] == 0:\n",
    "            del words1[i]\n",
    "            del freqs1[i]\n",
    "            len1 -= 1\n",
    "            del train_words[w]\n",
    "\n",
    "        prc += delta\n",
    "        sum_words -= delta\n",
    "        if (prc%100000) == 0:\n",
    "          print(prc)\n",
    "        \n",
    "    return train_words, test_words\n",
    "\n",
    "\n",
    "def makeNumericalVectorsAndSplit(word_freqs: dict, c_freqs: dict, percent: float) -> (list, list, list, list):\n",
    "    train_words, test_words = makeData4GeneratorsAndSplit(word_freqs, c_freqs, percent)\n",
    "    tx1, ty1 = makeNumericalVectors(train_words, c_freqs)\n",
    "    tx2, ty2 = makeNumericalVectors(test_words, c_freqs)\n",
    "    return tx1, tx2, ty1, ty2\n",
    "\n",
    "def makeNumericalVectorsAndSplit2(word_freqs: dict, c_freqs: dict, percent: float) -> (list, list, list, list):\n",
    "    tx1, ty1 = makeNumericalVectors(word_freqs, c_freqs)\n",
    "    return train_test_split(tx1, ty1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eMLQjBPkgce5"
   },
   "outputs": [],
   "source": [
    "# def endEpoche():\n",
    "#     train_dict2 = deepcopy(train_dict)\n",
    "\n",
    "def encodeString3(s) -> list:\n",
    "    res = np.zeros((len(c_freqs)*7, 1))\n",
    "    for i, c in enumerate(s):\n",
    "        res[c_freqs[c] + len(c_freqs) * i] = np.ones(1)\n",
    "    return res\n",
    "\n",
    "def generateOneBatch(keys):\n",
    "    batch = []\n",
    "    classes = []\n",
    "    for j in range(batch_size):\n",
    "#             for j in range(1):\n",
    "        index = random.randint(0, len(keys) - 1)\n",
    "        word = keys[index]\n",
    "        if word[3] in symbols_of_interest:\n",
    "            c = [1, 0]\n",
    "            word2 = word[:3] + replaces[word[3]] + word[4:]\n",
    "        else:\n",
    "            c = [0, 1]\n",
    "            word2 = word\n",
    "\n",
    "        batch.append(encodeString3(word2))\n",
    "        classes.append(c)\n",
    "        if train_dict2[word] == 1:\n",
    "            del keys[index]\n",
    "            #print(\"del\", len(keys))\n",
    "        else:\n",
    "            train_dict2[word] -= 1\n",
    "    return np.array(batch), np.array(classes), keys\n",
    "    \n",
    "def generateBatch():\n",
    "    while True:\n",
    "        dlen = int(sum(train_dict2.values()) / batch_size)\n",
    "        keys = list(train_dict2.keys())\n",
    "        for i in range(dlen):\n",
    "            batch, classes, keys = generateOneBatch(keys)\n",
    "            yield batch, classes\n",
    "\n",
    "def generateTestBatch():\n",
    "    while True:\n",
    "        dlen = int(sum(train_dict2.values()) / batch_size)\n",
    "        keys = list(train_dict2.keys())\n",
    "        for i in range(dlen):\n",
    "            batch, classes, keys = generateOneBatch(keys)\n",
    "        \n",
    "            if (i%100) == 0:\n",
    "                print(\"batch\", i)\n",
    "            global test_y\n",
    "            test_y.extend([n[0] for n in classes])\n",
    "            \n",
    "            yield batch, classes\n",
    "            \n",
    "            \n",
    "class MyCustomCallback(keras.callbacks.Callback):\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        global train_dict2, train_dict\n",
    "        train_dict2 = deepcopy(train_dict)\n",
    "        print(\"batch copied\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B2HUFxVKgeVc"
   },
   "outputs": [],
   "source": [
    "c_freqs = []\n",
    "\n",
    "def getDenseData(filename, soi):\n",
    "    word_freqs, c_freqs = extractNGrammsFromFile(filename, soi, 2)\n",
    "    #train_x, test_x, train_y, test_y =\n",
    "    return makeNumericalVectorsAndSplit(word_freqs, c_freqs, 0.2)\n",
    "\n",
    "def getDenseData2(filename, soi):\n",
    "    word_freqs, c_freqs = extractNGrammsFromFile(filename, soi, 2)\n",
    "    #train_x, test_x, train_y, test_y =\n",
    "    return makeNumericalVectorsAndSplit2(word_freqs, c_freqs, 0.2)\n",
    "\n",
    "def getGeneratorData(filename, soi):\n",
    "    global c_freqs\n",
    "    word_freqs, c_freqs = extractNGrammsFromFile(filename, soi, 2)\n",
    "    #train_dict, test_dict = \n",
    "    return makeData4GeneratorsAndSplit(word_freqs, c_freqs, 0.2)\n",
    "\n",
    "def getGeneratorData2(filename, soi):\n",
    "    global c_freqs\n",
    "    word_freqs, c_freqs = extractNGrammsFromFile(filename, soi, 2)\n",
    "    #train_dict, test_dict = \n",
    "    return makeData4GeneratorsAndSplit2(word_freqs, c_freqs, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UjkcwfEyggIs"
   },
   "outputs": [],
   "source": [
    "def getDenseModel():\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Dense(128, activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.001)))\n",
    "    model.add(layers.Dense(128, activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.001)))\n",
    "    model.add(layers.Dense(128, activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.001)))\n",
    "    model.add(layers.Dense(2, activation=\"softmax\"))\n",
    "    model.compile(keras.optimizers.Adam(learning_rate=0.001), \n",
    "              keras.losses.MeanSquaredError(reduction='sum'),\n",
    "              metrics=['accuracy']\n",
    "             )\n",
    "\n",
    "    return model\n",
    "\n",
    "def getConvModel():\n",
    "    model2 = keras.Sequential()\n",
    "    model2.add(layers.Conv1D(32, 3, activation='relu'))#,input_shape=(100, 7)))\n",
    "    model2.add(layers.Conv1D(32, 3, activation='relu'))\n",
    "    model2.add(layers.Flatten())\n",
    "    model2.add(layers.Dense(128, activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.001)))\n",
    "    model2.add(layers.Dense(2, activation=\"softmax\"))\n",
    "\n",
    "    model2.compile(keras.optimizers.Adam(learning_rate=0.001), keras.losses.BinaryCrossentropy(reduction='sum'))\n",
    "    \n",
    "    return model2\n",
    "\n",
    "def getLSTMModel():\n",
    "    model2 = keras.Sequential()\n",
    "    model2.add(layers.Dense(32, activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.001)))\n",
    "    # model2.add(layers.Bidirectional(layers.LSTM(128, activation='tanh')))\n",
    "    #model2.add(layers.Embedding(256, 128, input_length=len(c_freqs)*7))\n",
    "    model2.add(layers.Bidirectional(layers.LSTM(128)))\n",
    "    # model2.add(layers.Dropout(0.1))\n",
    "    model2.add(layers.Flatten())\n",
    "    #model2.add(layers.Dense(128, activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.001)))\n",
    "    model2.add(layers.Dense(2, activation=\"softmax\"))\n",
    "\n",
    "    model2.compile(keras.optimizers.Adam(learning_rate=0.001, clipnorm=1.), \n",
    "    #                keras.losses.BinaryCrossentropy(reduction='sum'),\n",
    "                   keras.losses.MeanSquaredError(reduction='sum'),\n",
    "                   metrics=[keras.metrics.Precision()])\n",
    "    \n",
    "    return model2\n",
    "\n",
    "def getLSTMModel2():\n",
    "    model2 = keras.Sequential()\n",
    "    #model2.add(layers.Dense(32, activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.001)))\n",
    "    #model2.add(layers.Embedding(256, 128, input_length=len(c_freqs)*7))\n",
    "    # model2.add(layers.Bidirectional(layers.LSTM(128, activation='tanh')))\n",
    "    model2.add(layers.Bidirectional(layers.LSTM(128)))\n",
    "    # model2.add(layers.Dropout(0.1))\n",
    "    model2.add(layers.Flatten())\n",
    "    #model2.add(layers.Dense(128, activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.001)))\n",
    "    model2.add(layers.Dense(2, activation=\"softmax\"))\n",
    "\n",
    "    model2.compile(keras.optimizers.Adam(learning_rate=0.001, clipnorm=1.), \n",
    "    #                keras.losses.BinaryCrossentropy(reduction='sum'),\n",
    "                   keras.losses.MeanSquaredError(reduction='sum'),\n",
    "                   metrics=[keras.metrics.Precision()])\n",
    "    \n",
    "    return model2\n",
    "\n",
    "\n",
    "#train_y3 = [(1,0) if y==0 else (0,1) for y in train_y]\n",
    "#test_y3 = [(1,0) if y==0 else (0,1) for y in test_y]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UW40eElrgh11"
   },
   "outputs": [],
   "source": [
    "def evaluate(y_real, y_hat):\n",
    "    to_print = str(confusion_matrix(y_real, y_hat)) + \"\\n\" + \\\n",
    "               \"recall =\" + str(recall_score(y_real, y_hat)) + \"\\n\" + \\\n",
    "               \"precision =\" + str(precision_score(y_real, y_hat)) + \"\\n\" + \\\n",
    "               \"accuracy =\" + str(accuracy_score(y_real, y_hat)) + \"\\n\" + \\\n",
    "               \"f1 =\" + str(f1_score(y_real, y_hat)) + \"\\n\"\n",
    "    return to_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K26WuHAvgji8"
   },
   "outputs": [],
   "source": [
    "turkish = {\"name\":\"turkish\", \"filename\":\"drive/My Drive/birgun[tr]-2019.txt\", \n",
    "           \"soi\":[\"uUüÜ\", \"sSşŞ\", \"cCçÇ\", \"oOöÖ\", \"gGğĞ\"]}\n",
    "french = {\"name\":\"french\", \"filename\":\"drive/My Drive/figaro_economics[fr]-2018.txt\", \n",
    "          # \"soi\":[\"eEéÉ\", \"eEèÈ\", \"aAàÀ\",  \"uUùÙ\",  \"aAâÂ\", \"eEêÊ\", \"iIîÎ\", \"oOôÔ\", \"uUûÛ\",\"cCçÇ\"]}\n",
    "          \"soi\":[\"iIîÎ\", \"oOôÔ\", \"uUûÛ\",\"cCçÇ\"]}\n",
    "german = {\"name\":\"german\", \"filename\":\"drive/My Drive/bild[de]-2018.txt\", \n",
    "          \"soi\":[\"aAäÄ\", \"oOöÖ\", \"uUüÜ\"]}\n",
    "croatian = {\"name\":\"croatian\", \"filename\": \"drive/My Drive/hr_200.txt\", \n",
    "           \"soi\": [\"cCčČ\", \"cCćĆ\", \"dDđÐ\", \"sSšŠ\", \"zZžŽ\"]}\n",
    "latvian = {\"name\": \"latvian\", \"filename\":\"drive/My Drive/lv_200.txt\",\n",
    "           \"soi\":[\"aAāĀ\", \"cCčČ\", \"eEēĒ\", \"gGģĢ\", \"iIīĪ\", \"kKķĶ\", \"lLļĻ\", \"nNņŅ\", \"sSšŠ\", \"uUūŪ\", \"zZžŽ\"]}\n",
    "romanian = {\"name\": \"romanian\", \"filename\": \"drive/My Drive/ro_200.txt\",\n",
    "            \"soi\": [\"aAăĂ\", \"aAâÂ\", \"iIîÎ\", \"sSșȘ\", \"tTțȚ\"]}\n",
    "slovak = {\"name\":\"slovak\", \"filename\":\"drive/My Drive/sk_200.txt\",\n",
    "#          \"soi\":[\"aAáÁ\", \"aAäÄ\", \"cCčČ\", \"dDďĎ\", \"eEéÉ\", \"iIíÍ\", \"lLĺĹ\", \"lLľĽ\", \"nNňŇ\", \"oOóÓ\", \"oOôÔ\", \"rRŕŔ\", \"sSšŠ\", \"tTťŤ\", \"uUúÚ\", \"yYýÝ\", \"zZžŽ\"]}\n",
    "          \"soi\":[\"nNňŇ\", \"oOóÓ\", \"oOôÔ\", \"rRŕŔ\", \"sSšŠ\", \"tTťŤ\", \"uUúÚ\", \"yYýÝ\", \"zZžŽ\"]}\n",
    "\n",
    "#languages = [turkish, french, german]\n",
    "#languages = [croatian, latvian, romanian, slovak]\n",
    "languages = [french]\n",
    "#languages = [latvian]\n",
    "#languages = [turkish]\n",
    "#languages = [slovak]\n",
    "#languages = [romanian]\n",
    "#languages = [croatian]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "36glcOhYhZMV",
    "outputId": "add5eab3-d1ba-4c08-f9fe-f5d1d51546db"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Replacement: 100%|██████████| 40/40 [00:09<00:00,  4.01it/s]\n",
      "Replacement: 100%|██████████| 16/16 [00:09<00:00,  1.77it/s]\n",
      "Creating n-gramms: 100%|██████████| 27262196/27262196 [00:48<00:00, 560516.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch copied\n",
      "Epoch 1/10\n",
      "11955/11955 [==============================] - 585s 49ms/step - loss: 8.7620\n",
      "batch copied\n",
      "Epoch 2/10\n",
      "11955/11955 [==============================] - 601s 50ms/step - loss: 8.6876\n",
      "batch copied\n",
      "Epoch 3/10\n",
      "11955/11955 [==============================] - 598s 50ms/step - loss: 8.6981\n",
      "batch copied\n",
      "Epoch 4/10\n",
      "11955/11955 [==============================] - 598s 50ms/step - loss: 8.6464\n",
      "batch copied\n",
      "Epoch 5/10\n",
      "11955/11955 [==============================] - 594s 50ms/step - loss: 8.6465\n",
      "batch copied\n",
      "Epoch 6/10\n",
      "11955/11955 [==============================] - 594s 50ms/step - loss: 8.6498\n",
      "batch copied\n",
      "Epoch 7/10\n",
      "11955/11955 [==============================] - 594s 50ms/step - loss: 8.6341\n",
      "batch copied\n",
      "Epoch 8/10\n",
      "11955/11955 [==============================] - 596s 50ms/step - loss: 8.6351\n",
      "batch copied\n",
      "Epoch 9/10\n",
      "11955/11955 [==============================] - 595s 50ms/step - loss: 11.1440\n",
      "batch copied\n",
      "Epoch 10/10\n",
      "11955/11955 [==============================] - 597s 50ms/step - loss: 4.3015\n",
      "should be 2996 batches\n",
      "batch 0\n",
      "batch 100\n",
      "batch 200\n",
      "batch 300\n",
      "batch 400\n",
      "batch 500\n",
      "batch 600\n",
      "batch 700\n",
      "batch 800\n",
      "batch 900\n",
      "batch 1000\n",
      "batch 1100\n",
      "batch 1200\n",
      "batch 1300\n",
      "batch 1400\n",
      "batch 1500\n",
      "batch 1600\n",
      "batch 1700\n",
      "batch 1800\n",
      "batch 1900\n",
      "batch 2000\n",
      "batch 2100\n",
      "batch 2200\n",
      "batch 2300\n",
      "batch 2400\n",
      "batch 2500\n",
      "batch 2600\n",
      "batch 2700\n",
      "batch 2800\n",
      "batch 2900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "language = slovak, soi=rRŕŔ\n",
      "[[1531538       0]\n",
      " [   2414       0]]\n",
      "recall =0.0\n",
      "precision =0.0\n",
      "accuracy =0.9984262871328438\n",
      "f1 =0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Replacement: 100%|██████████| 40/40 [00:09<00:00,  4.03it/s]\n",
      "Replacement: 100%|██████████| 16/16 [00:09<00:00,  1.73it/s]\n",
      "Creating n-gramms:  97%|█████████▋| 26329552/27262196 [00:47<00:01, 573136.04it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-7a36d1adffb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlang\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlanguages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msoi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"soi\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mtrain_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetGeneratorData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"filename\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m10000000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-7d04fa921504>\u001b[0m in \u001b[0;36mgetGeneratorData\u001b[0;34m(filename, soi)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetGeneratorData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mc_freqs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mword_freqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_freqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextractNGrammsFromFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;31m#train_dict, test_dict =\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmakeData4GeneratorsAndSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_freqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_freqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-b1004816696c>\u001b[0m in \u001b[0;36mextractNGrammsFromFile\u001b[0;34m(filename, soi, freq_thr)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mword_freqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Creating n-gramms'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mcur\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"   \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "outfilename = \"evaluation34.txt\"\n",
    "\n",
    "for lang in languages:\n",
    "    for soi in lang[\"soi\"]:\n",
    "        train_dict, test_dict = getGeneratorData(lang[\"filename\"], soi)\n",
    "\n",
    "        if sum(train_dict.values()) > 10000000:\n",
    "          for k, v in tqdm(train_dict.items(), desc = \"Reducing frequencies\"):\n",
    "            if v > 5000:\n",
    "              train_dict[k] = int(train_dict[k] / 2)\n",
    "            elif v > 1000:\n",
    "              train_dict[k] = int(train_dict[k] / 1.5)\n",
    "            elif v > 500:\n",
    "              train_dict[k] = int(train_dict[k] / 1.3)\n",
    "            elif v > 100:\n",
    "              train_dict[k] = int(train_dict[k] / 1.2)\n",
    "\n",
    "\n",
    "        #model = getLSTMModel()\n",
    "        model = getConvModel()\n",
    "\n",
    "        train_dict2 = deepcopy(train_dict)\n",
    "        batch_size = 512\n",
    "        steps_per_epoche = int(sum(train_dict.values()) / batch_size) - 1\n",
    "\n",
    "        model.fit(generateBatch(), steps_per_epoch=steps_per_epoche, epochs=10, callbacks=[MyCustomCallback()])        \n",
    "        #model.summary()\n",
    "        \n",
    "        train_dict2 = deepcopy(test_dict)\n",
    "        steps_per_epoche = int(sum(train_dict2.values()) / batch_size) - 1\n",
    "        test_y = []\n",
    "        print(f\"should be {steps_per_epoche} batches\")\n",
    "        yn_hat3 = model.predict(generateTestBatch(), steps=steps_per_epoche)\n",
    "\n",
    "        yn_hat4 = [0 if y[0]<y[1] else 1 for y in yn_hat3]\n",
    "        test_y = test_y[:len(yn_hat4)]\n",
    "        \n",
    "        eva = evaluate(test_y, yn_hat4)\n",
    "        print(f\"\\nlanguage = {lang['name']}, soi={soi}\")\n",
    "        print(eva)\n",
    "        s = f\"------\\nConvolution 2x3, one-hot data\\n language = {lang['name']}, soi={soi}\\n\"+eva\n",
    "\n",
    "        out_file = open(outfilename, \"a\")\n",
    "        out_file.write(s)\n",
    "        out_file.close()      \n",
    "\n",
    "\n",
    "# with open(outfilename) as file:\n",
    "#   text = file.read()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 590
    },
    "colab_type": "code",
    "id": "qnL9xWtWiSuf",
    "outputId": "cfc523bc-7d0c-4795-8efd-5e21f0fb70ae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Replacement: 100%|██████████| 40/40 [00:08<00:00,  4.64it/s]\n",
      "Replacement: 100%|██████████| 16/16 [00:08<00:00,  1.92it/s]\n",
      "Creating n-gramms: 100%|██████████| 20904669/20904669 [00:24<00:00, 845345.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch copied\n",
      "Epoch 1/10\n",
      "10910/10910 [==============================] - 225s 21ms/step - loss: 1.6296\n",
      "batch copied\n",
      "Epoch 2/10\n",
      "10910/10910 [==============================] - 242s 22ms/step - loss: 0.9154\n",
      "batch copied\n",
      "Epoch 3/10\n",
      "10910/10910 [==============================] - 245s 22ms/step - loss: 33.2815\n",
      "batch copied\n",
      "Epoch 4/10\n",
      "10910/10910 [==============================] - 245s 22ms/step - loss: 33.4838\n",
      "batch copied\n",
      "Epoch 5/10\n",
      "10910/10910 [==============================] - 242s 22ms/step - loss: 33.2668\n",
      "batch copied\n",
      "Epoch 6/10\n",
      "10910/10910 [==============================] - 245s 22ms/step - loss: 102.3654\n",
      "batch copied\n",
      "Epoch 7/10\n",
      "10910/10910 [==============================] - 247s 23ms/step - loss: 33.2657\n",
      "batch copied\n",
      "Epoch 8/10\n",
      "10910/10910 [==============================] - 248s 23ms/step - loss: 33.2653\n",
      "batch copied\n",
      "Epoch 9/10\n",
      "10910/10910 [==============================] - 246s 23ms/step - loss: 33.2652\n",
      "batch copied\n",
      "Epoch 10/10\n",
      " 9202/10910 [========================>.....] - ETA: 38s - loss: 39.4396"
     ]
    }
   ],
   "source": [
    "outfilename = \"drive/My Drive/evaluation50.txt\"\n",
    "\n",
    "for lang in languages:\n",
    "    for soi in lang[\"soi\"]:\n",
    "        train_dict, test_dict = getGeneratorData2(lang[\"filename\"], soi)\n",
    "\n",
    "        if sum(train_dict.values()) > 10000000:\n",
    "          for k, v in tqdm(train_dict.items(), desc = \"Reducing frequencies\"):\n",
    "            if v > 5000:\n",
    "              train_dict[k] = int(train_dict[k] / 2)\n",
    "            elif v > 1000:\n",
    "              train_dict[k] = int(train_dict[k] / 1.5)\n",
    "            elif v > 500:\n",
    "              train_dict[k] = int(train_dict[k] / 1.3)\n",
    "            elif v > 100:\n",
    "              train_dict[k] = int(train_dict[k] / 1.2)\n",
    "\n",
    "\n",
    "        #model = getLSTMModel()\n",
    "        model = getConvModel()\n",
    "\n",
    "        train_dict2 = deepcopy(train_dict)\n",
    "        batch_size = 512\n",
    "        steps_per_epoche = int(sum(train_dict.values()) / batch_size) - 1\n",
    "\n",
    "        model.fit(generateBatch(), steps_per_epoch=steps_per_epoche, epochs=10, callbacks=[MyCustomCallback()])        \n",
    "        #model.summary()\n",
    "        \n",
    "        train_dict2 = deepcopy(test_dict)\n",
    "        steps_per_epoche = int(sum(train_dict2.values()) / batch_size) - 1\n",
    "        test_y = []\n",
    "        print(f\"should be {steps_per_epoche} batches\")\n",
    "        yn_hat3 = model.predict(generateTestBatch(), steps=steps_per_epoche)\n",
    "\n",
    "        yn_hat4 = [0 if y[0]<y[1] else 1 for y in yn_hat3]\n",
    "        test_y = test_y[:len(yn_hat4)]\n",
    "        \n",
    "        eva = evaluate(test_y, yn_hat4)\n",
    "        print(f\"\\nlanguage = {lang['name']}, soi={soi}\")\n",
    "        print(eva)\n",
    "        s = f\"------\\nConvtolution 2x3, one-hot, mixed words\\n language = {lang['name']}, soi={soi}\\n\"+eva\n",
    "\n",
    "        out_file = open(outfilename, \"a\")\n",
    "        out_file.write(s)\n",
    "        out_file.close()      \n",
    "\n",
    "\n",
    "# with open(outfilename) as file:\n",
    "#   text = file.read()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "xVK2ULyFVMJ8",
    "outputId": "1ca495ce-45f2-4230-ac06-6816c37bfa62"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Replacement: 100%|██████████| 40/40 [00:12<00:00,  3.25it/s]\n",
      "Replacement: 100%|██████████| 16/16 [00:10<00:00,  1.46it/s]\n",
      "Creating n-gramms: 100%|██████████| 30767325/30767325 [00:42<00:00, 724574.77it/s]\n",
      "Vectorizing words: 100%|██████████| 115287/115287 [00:02<00:00, 40469.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5625/5625 [==============================] - 54s 10ms/step - loss: 33.0216 - precision: 0.9791 - val_loss: 24.7281 - val_precision: 0.9854\n",
      "Epoch 2/10\n",
      "5625/5625 [==============================] - 53s 9ms/step - loss: 23.1646 - precision: 0.9861 - val_loss: 20.7713 - val_precision: 0.9873\n",
      "Epoch 3/10\n",
      "5625/5625 [==============================] - 53s 9ms/step - loss: 20.4191 - precision: 0.9874 - val_loss: 19.7455 - val_precision: 0.9880\n",
      "Epoch 4/10\n",
      "5625/5625 [==============================] - 54s 10ms/step - loss: 19.2723 - precision: 0.9880 - val_loss: 18.9008 - val_precision: 0.9881\n",
      "Epoch 5/10\n",
      "5625/5625 [==============================] - 54s 10ms/step - loss: 18.7102 - precision: 0.9883 - val_loss: 18.4899 - val_precision: 0.9885\n",
      "Epoch 6/10\n",
      "5625/5625 [==============================] - 54s 10ms/step - loss: 18.3494 - precision: 0.9885 - val_loss: 17.9996 - val_precision: 0.9887\n",
      "Epoch 7/10\n",
      "5625/5625 [==============================] - 53s 10ms/step - loss: 17.6327 - precision: 0.9889 - val_loss: 17.1311 - val_precision: 0.9892\n",
      "Epoch 8/10\n",
      "5625/5625 [==============================] - 53s 9ms/step - loss: 17.1116 - precision: 0.9892 - val_loss: 17.0786 - val_precision: 0.9892\n",
      "Epoch 9/10\n",
      "5625/5625 [==============================] - 53s 9ms/step - loss: 17.0121 - precision: 0.9892 - val_loss: 16.9839 - val_precision: 0.9891\n",
      "Epoch 10/10\n",
      "5625/5625 [==============================] - 53s 9ms/step - loss: 16.8991 - precision: 0.9893 - val_loss: 16.8793 - val_precision: 0.9890\n",
      "language=romanian, soi=iIîÎ\n",
      "\n",
      "[[3592423   19622]\n",
      " [  22454  205433]]\n",
      "recall =0.901468710369613\n",
      "precision =0.9128124236297794\n",
      "accuracy =0.989042514294524\n",
      "f1 =0.9071051039647461\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Replacement: 100%|██████████| 40/40 [00:12<00:00,  3.25it/s]\n",
      "Replacement: 100%|██████████| 16/16 [00:10<00:00,  1.48it/s]\n",
      "Creating n-gramms: 100%|██████████| 30767325/30767325 [00:39<00:00, 783992.61it/s]\n",
      "Vectorizing words: 100%|██████████| 41674/41674 [00:00<00:00, 42370.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2012/2012 [==============================] - 20s 10ms/step - loss: 0.4678 - precision_1: 0.9997 - val_loss: 8.9211e-09 - val_precision_1: 1.0000\n",
      "Epoch 2/10\n",
      "2012/2012 [==============================] - 20s 10ms/step - loss: 8.7109e-09 - precision_1: 1.0000 - val_loss: 8.5920e-09 - val_precision_1: 1.0000\n",
      "Epoch 3/10\n",
      "2012/2012 [==============================] - 19s 10ms/step - loss: 8.1761e-09 - precision_1: 1.0000 - val_loss: 7.7674e-09 - val_precision_1: 1.0000\n",
      "Epoch 4/10\n",
      "2012/2012 [==============================] - 19s 10ms/step - loss: 7.0425e-09 - precision_1: 1.0000 - val_loss: 6.2644e-09 - val_precision_1: 1.0000\n",
      "Epoch 5/10\n",
      "2012/2012 [==============================] - 19s 10ms/step - loss: 5.1993e-09 - precision_1: 1.0000 - val_loss: 4.1482e-09 - val_precision_1: 1.0000\n",
      "Epoch 6/10\n",
      "2012/2012 [==============================] - 19s 10ms/step - loss: 3.1593e-09 - precision_1: 1.0000 - val_loss: 2.2974e-09 - val_precision_1: 1.0000\n",
      "Epoch 7/10\n",
      "2012/2012 [==============================] - 19s 9ms/step - loss: 1.6334e-09 - precision_1: 1.0000 - val_loss: 1.1090e-09 - val_precision_1: 1.0000\n",
      "Epoch 8/10\n",
      "2012/2012 [==============================] - 19s 9ms/step - loss: 7.7926e-10 - precision_1: 1.0000 - val_loss: 5.2267e-10 - val_precision_1: 1.0000\n",
      "Epoch 9/10\n",
      "2012/2012 [==============================] - 19s 10ms/step - loss: 3.6810e-10 - precision_1: 1.0000 - val_loss: 2.5020e-10 - val_precision_1: 1.0000\n",
      "Epoch 10/10\n",
      "2012/2012 [==============================] - 19s 10ms/step - loss: 1.7921e-10 - precision_1: 1.0000 - val_loss: 1.2255e-10 - val_precision_1: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1515: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  average, \"true nor predicted\", 'F-score is', len(true_sum)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language=romanian, soi=sSșȘ\n",
      "\n",
      "[[1372908]]\n",
      "recall =0.0\n",
      "precision =0.0\n",
      "accuracy =1.0\n",
      "f1 =0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Replacement: 100%|██████████| 40/40 [00:12<00:00,  3.28it/s]\n",
      "Replacement: 100%|██████████| 16/16 [00:10<00:00,  1.52it/s]\n",
      "Creating n-gramms: 100%|██████████| 30767325/30767325 [00:39<00:00, 785175.97it/s]\n",
      "Vectorizing words: 100%|██████████| 65917/65917 [00:02<00:00, 31482.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3867/3867 [==============================] - 38s 10ms/step - loss: 63.3687 - precision_2: 0.9642 - val_loss: 41.0960 - val_precision_2: 0.9777\n",
      "Epoch 2/10\n",
      "3867/3867 [==============================] - 37s 10ms/step - loss: 34.2639 - precision_2: 0.9808 - val_loss: 30.9326 - val_precision_2: 0.9821\n",
      "Epoch 3/10\n",
      "3867/3867 [==============================] - 37s 9ms/step - loss: 29.3729 - precision_2: 0.9836 - val_loss: 27.5333 - val_precision_2: 0.9846\n",
      "Epoch 4/10\n",
      "3867/3867 [==============================] - 37s 10ms/step - loss: 26.4472 - precision_2: 0.9852 - val_loss: 26.4573 - val_precision_2: 0.9849\n",
      "Epoch 5/10\n",
      "3867/3867 [==============================] - 37s 10ms/step - loss: 24.3408 - precision_2: 0.9862 - val_loss: 23.2735 - val_precision_2: 0.9867\n",
      "Epoch 6/10\n",
      "3867/3867 [==============================] - 37s 9ms/step - loss: 22.9804 - precision_2: 0.9869 - val_loss: 22.3137 - val_precision_2: 0.9873\n",
      "Epoch 7/10\n",
      "3867/3867 [==============================] - 37s 9ms/step - loss: 22.3523 - precision_2: 0.9873 - val_loss: 22.0945 - val_precision_2: 0.9873\n",
      "Epoch 8/10\n",
      "3867/3867 [==============================] - 37s 10ms/step - loss: 20.7191 - precision_2: 0.9880 - val_loss: 20.0320 - val_precision_2: 0.9884\n",
      "Epoch 9/10\n",
      "3867/3867 [==============================] - 37s 9ms/step - loss: 20.2933 - precision_2: 0.9882 - val_loss: 20.3216 - val_precision_2: 0.9880\n",
      "Epoch 10/10\n",
      "3867/3867 [==============================] - 36s 9ms/step - loss: 19.9239 - precision_2: 0.9884 - val_loss: 19.5681 - val_precision_2: 0.9886\n",
      "language=romanian, soi=tTțȚ\n",
      "\n",
      "[[2419529   12358]\n",
      " [  17659  190230]]\n",
      "recall =0.9150556306490483\n",
      "precision =0.938999348431299\n",
      "accuracy =0.9886289594268605\n",
      "f1 =0.9268728820372396\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outfilename = \"drive/My Drive/evaluation39.txt\"\n",
    "\n",
    "for lang in languages:\n",
    "    for soi in lang[\"soi\"]:\n",
    "        train_x, test_x, train_y, test_y = getDenseData2(lang[\"filename\"], soi)\n",
    "        train_y3 = [(1,0) if y==0 else (0,1) for y in train_y]\n",
    "        test_y3 = [(1,0) if y==0 else (0,1) for y in test_y]\n",
    "\n",
    "        d1 = np.array(train_x, dtype=np.float32)\n",
    "        d1 = d1.reshape(-1, 7, 1)\n",
    "        d2 = np.array(train_y3, dtype=np.float32)\n",
    "        d3 = np.array(test_x, dtype=np.float32)\n",
    "        d3 = d3.reshape(-1, 7, 1)\n",
    "        d4 = np.array(test_y3, dtype=np.float32)\n",
    "\n",
    "        model = getLSTMModel2()\n",
    "        #model = getLSTMModel()\n",
    "        #model = getConvModel()\n",
    "        model.fit(d1, d2, batch_size=2048, epochs=10, validation_data=(d3, d4))\n",
    "        #model.fit(train_x, train_y3, batch_size=2048, epochs=15, validation_data=(test_x, test_y3))\n",
    "        #model.summary()\n",
    "\n",
    "        yn_hat = model.predict(d3)\n",
    "        yn_hat2 = [0 if y[0]>y[1] else 1 for y in yn_hat]\n",
    "\n",
    "        #ynt_hat = model.predict(test_x)\n",
    "        #yn_hat2 = [0 if y[0]>y[1] and y[0]>0.7 else 1 for y in ynt_hat]\n",
    "\n",
    "        eva = evaluate(test_y, yn_hat2)\n",
    "        print(f\"language={lang['name']}, soi={soi}\")\n",
    "        print(\"\\n\" + eva)\n",
    "        \n",
    "        out_file = open(outfilename, \"a\")\n",
    "        s = f\"------\\nLSTM x 128, dense data, mixed words\\n language = {lang['name']}, soi={soi}\\n\"+eva\n",
    "        \n",
    "        out_file.write(s)\n",
    "        out_file.close()    \n",
    "\n",
    "#with open(outfilename) as file:\n",
    "#  text = file.read()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "8Shzqqfikm8p",
    "outputId": "b28b7f98-17a8-44e9-9b79-2e6555d042f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
